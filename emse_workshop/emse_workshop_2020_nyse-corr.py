# -*- coding: utf-8 -*-
"""EMSE - Workshop 2020 - NYSE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15TOsNqD_CMvic7DlSrdtL71masBQuzWC

## NYSE

Original notebook: https://www.kaggle.com/raoulma/ny-stock-price-prediction-rnn-lstm-gru



First, let's download data.
"""

!pip install gdown

!gdown https://drive.google.com/uc?id=1BsBVz1QCN3-KQnrHRx7tFm84yNrZdSdi

"""We unzip (silently) data and delete zip file afterward."""

!unzip -qo 'nyse.zip'
!rm 'nyse.zip'

!ls

"""Import, load data and first glance"""

import numpy as np
import pandas as pd
import math
import sklearn
import sklearn.preprocessing
import datetime
import os
import matplotlib.pyplot as plt
import tensorflow as tf

# split data in 80%/10%/10% train/validation/test sets
valid_set_size_percentage = 10
test_set_size_percentage = 10

# import all stock prices
df = pd.read_csv("prices-split-adjusted.csv", index_col = 0)
df.info()
df.head()

# number of different stocks
print('\nnumber of different stocks: ', len(list(set(df.symbol))))
print(list(set(df.symbol))[:10])
print('=' * 40)

print(df.describe())
print('=' * 40)

print(df.info())

plt.figure(figsize=(15, 10));
plt.subplot(2, 1, 1)
plt.plot(df[df.symbol == 'EQIX'].open.values, color='red', label='open')
plt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')
plt.plot(df[df.symbol == 'EQIX'].low.values, color='blue', label='low')
plt.plot(df[df.symbol == 'EQIX'].high.values, color='black', label='high')
plt.title('stock price')
plt.xlabel('time [days]')
plt.ylabel('price')
plt.legend(loc='best')

plt.subplot(2, 1, 2);
plt.plot(df[df.symbol == 'EQIX'].volume.values, color='black', label='volume')
plt.title('stock volume')
plt.xlabel('time [days]')
plt.ylabel('volume')
plt.legend(loc='best')
plt.show()

# choose one stock
df_stock = df[df.symbol == 'EQIX'].copy()
df_stock.drop(['symbol'],1,inplace=True)
# Comment next line to use volume
df_stock.drop(['volume'],1,inplace=True)

cols = list(df_stock.columns.values)
print('df_stock.columns.values = ', cols)

df_stock_norm = df_stock.copy()

# Need to cut NOW
nbs = df_stock_norm.values.shape[0]
valid_set_size = int(np.round(valid_set_size_percentage/100*nbs));  
test_set_size = int(np.round(test_set_size_percentage/100*nbs));
train_set_size = nbs - (valid_set_size + test_set_size);

# ben on coupe!
data_raw = df_stock_norm.values
x_train = data_raw[:train_set_size]
x_valid = data_raw[train_set_size:train_set_size + valid_set_size]
x_test = data_raw[train_set_size + valid_set_size:]

if False:
  x_train = np.concatenate((np.zeros((1, x_train.shape[1])),
                            x_train[1:] - x_train[:-1]))
  x_valid = np.concatenate((np.zeros((1, x_valid.shape[1])),
                            x_valid[1:] - x_valid[:-1]))
  x_test = np.concatenate((np.zeros((1, x_test.shape[1])),
                            x_test[1:] - x_test[:-1]))

if True:
  epsilon = 1e-8
  x_train = np.concatenate((np.ones((1, x_train.shape[1])),
                            x_train[1:] / (x_train[:-1] + epsilon)))
  x_valid = np.concatenate((np.ones((1, x_valid.shape[1])),
                            x_valid[1:] / (x_valid[:-1] + epsilon)))
  x_test = np.concatenate((np.ones((1, x_test.shape[1])),
                          x_test[1:] / (x_test[:-1] + epsilon)))

# normalize stock
min_max_scaler = sklearn.preprocessing.MinMaxScaler()
min_max_scaler.fit(x_train)
x_train = min_max_scaler.transform(x_train)
x_valid = min_max_scaler.transform(x_valid)
x_test = min_max_scaler.transform(x_test)

# create train, test data
seq_len = 20 # choose sequence length

# create all possible sequences of length seq_len
data = []
for index in range(x_train.shape[0] - seq_len): 
    data.append(x_train[index: index + seq_len])
x_train = np.array(data);

data = []
for index in range(x_valid.shape[0] - seq_len): 
    data.append(x_valid[index: index + seq_len])
x_valid = np.array(data);

data = []
for index in range(x_test.shape[0] - seq_len): 
    data.append(x_test[index: index + seq_len])
x_test = np.array(data);

y_train = x_train[:, -1, :]
y_valid = x_valid[:, -1, :]
y_test = x_test[:, -1, :]

x_train = x_train[:, :-1, :]
x_valid = x_valid[:, :-1, :]
x_test = x_test[:, :-1, :]

print('x_train.shape = ',x_train.shape)
print('y_train.shape = ', y_train.shape)
print('x_valid.shape = ',x_valid.shape)
print('y_valid.shape = ', y_valid.shape)
print('x_test.shape = ', x_test.shape)
print('y_test.shape = ',y_test.shape)

plt.figure(figsize=(15, 5))
fig, ax1 = plt.subplots()
ax1.plot(df_stock_norm.open.values, color='red', label='open')
ax1.plot(df_stock_norm.close.values, color='green', label='low')
ax1.plot(df_stock_norm.low.values, color='blue', label='low')
ax1.plot(df_stock_norm.high.values, color='black', label='high')
ax1.set_ylabel('normalized price')
if 'volume' in df_stock_norm:
  ax2 = ax1.twinx()
  ax2.plot(df_stock_norm.volume.values, color='gray', label='volume')
  ax2.set_ylabel('normalized volume')
plt.title('stock')
plt.xlabel('time [days]')
plt.legend(loc='best')
plt.show()

st_zoom, si_zoom = 0, 200
zoom = np.arange(st_zoom, st_zoom + si_zoom).astype(np.int)
plt.figure(figsize=(15, 5))
fig, ax1 = plt.subplots()
ax1.plot(df_stock_norm.open.values[zoom], color='red', label='open')
ax1.plot(df_stock_norm.close.values[zoom], color='green', label='low')
ax1.plot(df_stock_norm.low.values[zoom], color='blue', label='low')
ax1.plot(df_stock_norm.high.values[zoom], color='black', label='high')
ax1.set_ylabel('normalized price')
if 'volume' in df_stock_norm:
  ax2 = ax1.twinx()
  ax2.plot(df_stock_norm.volume.values[zoom], color='gray', label='volume')
  ax2.set_ylabel('normalized volume')
plt.title('stock')
plt.xlabel('time [days]')
plt.legend(loc='best')
plt.show()

# parameters
n_steps = seq_len-1 
n_inputs = 4 if 'volume' not in df_stock_norm else 5
n_neurons = 200 
n_outputs = 4 if 'volume' not in df_stock_norm else 5
n_layers = 2
learning_rate = 0.001
batch_size = 50
n_epochs = 100

## Basic Cell RNN in tensorflow

index_in_epoch = 0;
perm_array  = np.arange(x_train.shape[0])
np.random.shuffle(perm_array)

# function to get the next batch
def get_next_batch(batch_size):
    global index_in_epoch, x_train, perm_array   
    start = index_in_epoch
    index_in_epoch += batch_size
    
    if index_in_epoch > x_train.shape[0]:
        np.random.shuffle(perm_array) # shuffle permutation array
        start = 0 # start next epoch
        index_in_epoch = batch_size
        
    end = index_in_epoch
    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]

train_set_size = x_train.shape[0]
test_set_size = x_test.shape[0]

tf.reset_default_graph()

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_outputs])

# use Basic RNN Cell
layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)
          for layer in range(n_layers)]

# use Basic LSTM Cell 
#layers = [tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons, activation=tf.nn.elu)
#          for layer in range(n_layers)]

# use LSTM Cell with peephole connections
#layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, 
#                                  activation=tf.nn.leaky_relu, use_peepholes = True)
#          for layer in range(n_layers)]

# use GRU cell
#layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)
#          for layer in range(n_layers)]
                                                                     
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)

stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = outputs[:,n_steps-1,:] # keep only last output of sequence
                                              
loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error 
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
training_op = optimizer.minimize(loss)
                                              
# run graph
with tf.Session() as sess: 
    sess.run(tf.global_variables_initializer())
    for iteration in range(int(n_epochs*train_set_size/batch_size)):
        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch 
        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) 
        if iteration % int(5*train_set_size/batch_size) == 0:
            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) 
            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) 
            print('%.2f epochs: MSE train/valid = %.6f/%.6f'%(
                iteration*batch_size/train_set_size, mse_train, mse_valid))

    y_train_pred = sess.run(outputs, feed_dict={X: x_train})
    y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})
    y_test_pred = sess.run(outputs, feed_dict={X: x_test})

def results(x_train, y_train, x_valid, y_valid, x_test, y_test,
            y_train_pred, y_valid_pred, y_test_pred):
  ft = 0 # 0 = open, 1 = close, 2 = highest, 3 = lowest

  tr, va, te = y_train, y_valid, y_test
  trp, vap, tep = y_train_pred, y_valid_pred, y_test_pred
  if False:
    tr, va, te = np.cumsum(tr, axis=0), np.cumsum(va, axis=0), np.cumsum(te, axis=0)
    trp, vap, tep = np.cumsum(trp, axis=0), np.cumsum(vap, axis=0), np.cumsum(tep, axis=0)
  if True:
    tr, va, te = np.cumprod(tr, axis=0), np.cumprod(va, axis=0), np.cumprod(te, axis=0)
    trp, vap, tep = np.cumprod(trp, axis=0), np.cumprod(vap, axis=0), np.cumprod(tep, axis=0)
    
  corr_price_development_train = np.mean((tr[:, 0] < tr[:, 1]) == (trp[:, 0] < trp[:, 1]))
  corr_price_development_valid = np.mean((va[:, 0] < va[:, 1]) == (vap[:, 0] < vap[:, 1]))
  corr_price_development_test = np.mean((te[:, 0] < te[:, 1]) == (tep[:, 0] < tep[:, 1]))

  ## show predictions
  plt.figure(figsize=(15, 10));
  plt.subplot(2, 1, 1);

  plt.plot(np.arange(y_train.shape[0]), y_train[:,ft], color='blue', label='train target')

  plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_valid.shape[0]), y_valid[:,ft],
          color='gray', label='valid target')

  plt.plot(np.arange(y_train.shape[0]+y_valid.shape[0],
                    y_train.shape[0]+y_test.shape[0]+y_test.shape[0]),
          y_test[:,ft], color='black', label='test target')

  plt.plot(np.arange(y_train_pred.shape[0]),y_train_pred[:,ft], color='red',
          label='train prediction')

  plt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_valid_pred.shape[0]),
          y_valid_pred[:,ft], color='orange', label='valid prediction')

  plt.plot(np.arange(y_train_pred.shape[0]+y_valid_pred.shape[0],
                    y_train_pred.shape[0]+y_valid_pred.shape[0]+y_test_pred.shape[0]),
          y_test_pred[:,ft], color='green', label='test prediction')

  plt.title('past and future stock prices')
  plt.xlabel('time [days]')
  plt.ylabel('normalized price')
  plt.legend(loc='best');

  plt.subplot(2, 1, 2);

  plt.plot(np.arange(y_train.shape[0], y_train.shape[0]+y_test.shape[0]),
          y_test[:,ft], color='black', label='test target')

  plt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_test_pred.shape[0]),
          y_test_pred[:,ft], color='green', label='test prediction')

  plt.title('future stock prices')
  plt.xlabel('time [days]')
  plt.ylabel('normalized price')
  plt.legend(loc='best');

  print('correct sign prediction for close - open price for train/valid/test: %.2f/%.2f/%.2f'%(
      corr_price_development_train, corr_price_development_valid, corr_price_development_test))

results(x_train, y_train, x_valid, y_valid, x_test, y_test,
        y_train_pred, y_valid_pred, y_test_pred)

"""Let's do the same network using Keras"""

lin = tf.keras.layers.Input(shape=(n_steps, n_inputs))
lact = lin

lact = tf.keras.layers.ZeroPadding1D(1)(lact)
lact = tf.keras.layers.Conv1D(24, 3, use_bias=False)(lact)
lact = tf.keras.layers.BatchNormalization()(lact)
lact = tf.keras.layers.Activation('relu')(lact)
lact = tf.keras.layers.SpatialDropout1D(4/24)(lact)

lact = tf.keras.layers.ZeroPadding1D(1)(lact)
lact = tf.keras.layers.Conv1D(32, 3, use_bias=False)(lact)
lact = tf.keras.layers.BatchNormalization()(lact)
lact = tf.keras.layers.Activation('relu')(lact)
lact = tf.keras.layers.SpatialDropout1D(6/32)(lact)

lact = tf.keras.layers.ZeroPadding1D(1)(lact)
lact = tf.keras.layers.Conv1D(40, 3, use_bias=False)(lact)
lact = tf.keras.layers.BatchNormalization()(lact)
lact = tf.keras.layers.Activation('relu')(lact)
lact = tf.keras.layers.SpatialDropout1D(8/40)(lact)

lact = tf.keras.layers.GlobalAveragePooling1D()(lact)
lout = tf.keras.layers.Dense(n_outputs, activation='linear')(lact)

model = tf.keras.models.Model(lin, lout)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
              loss=tf.keras.losses.MSE)

model.fit(x_train, y_train, validation_data=(x_valid, y_valid),
          epochs=n_epochs, batch_size=batch_size)

model.summary()

y_train_pred = model.predict(x_train)
  y_valid_pred = model.predict(x_valid)
  y_test_pred = model.predict(x_test)

results(x_train, y_train, x_valid, y_valid, x_test, y_test,
        y_train_pred, y_valid_pred, y_test_pred)

